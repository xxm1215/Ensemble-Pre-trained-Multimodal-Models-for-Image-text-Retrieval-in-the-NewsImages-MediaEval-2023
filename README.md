# Ensemble-Pre-trained-Multimodal-Models-for-Image-text-Retrieval-in-the-NewsImages-MediaEval-2023
Run #1：使用Blip-2模型作为预训练模型，分别对文本和图像进行编码获取各自的特征，通过计算文本与所有图像的余弦相似度进行排名，在排名结果中选择前100张最相关的图像作为我们的预测结果。
Run #2：使用CLIP的ViT-H/14模型作为预训练模型，分别对文章文本和图像进行编码，计算文章文本特征与所有测试图像特征之间的相似度，考虑增加Dual softmax方法来计算文本和图像之间的相似度排名，选取前100张最相关的图像作为我们的预测结果。
Run #3：通过设计多任务对比学习模型，对测试集的处理与训练集类似，将每一条文本与测试集中所有图片计算余弦相似度，最终只保留相似度排名前100的图文对作为我们的预测结果。
Run #4：与Run #2类似，我们使用CLIP的ViT-H/14@336px模型，分别对文章文本（或文章标题）和图像进行编码。
Run #5：在Run #2、Run #3和Run #4的基础上，我们重新训练了三个模型，考虑将三个模型的预测结果集成进行投票得到最终的决策，其中每个模型的结果包含所有的文本，每条文本对应的100张图像以及每条文本和图像之间的余弦相似度，随后选定一条文本的URL，把所有相同的图片的余弦相似度加和，将结果进行从大到小排序，选取前100张最相关的图像作为我们的预测结果。
